{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "85c64e09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: tqdm in c:\\users\\desktop\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (4.67.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\desktop\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from tqdm) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.3\n",
      "[notice] To update, run: C:\\Users\\Desktop\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5d5d7034",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import faiss  # IMPORTANTE: Biblioteca FAISS para otimização\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MultiLabelBinarizer\n",
    "from sklearn.metrics import classification_report, multilabel_confusion_matrix\n",
    "from tqdm import tqdm\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64f8024e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concatenando datasets...\n",
      "Sanitizando dados numéricos (removendo cabeçalhos repetidos e strings inválidas)...\n",
      "   AVISO: Removendo 1 linhas inválidas que não puderam ser convertidas para float.\n",
      "Dados sanitizados com sucesso.\n",
      "\n",
      "========================================\n",
      "       RESUMO DO DATASET (Limpo)       \n",
      "========================================\n",
      "Shape Total: (2989048, 22) (Linhas, Colunas)\n",
      "\n",
      "Distribuição de Rótulos (Top 20):\n",
      "Label\n",
      "Benign                      2596445\n",
      "DoS attacks-Hulk             145009\n",
      "Bot                          143967\n",
      "Infilteration                 51834\n",
      "DoS attacks-GoldenEye         41356\n",
      "DoS attacks-Slowloris          9859\n",
      "Brute Force -Web                337\n",
      "Brute Force -XSS                143\n",
      "DoS attacks-SlowHTTPTest         55\n",
      "SQL Injection                    43\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Informações das Colunas:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 2989048 entries, 192-152-9425.666667-0.0-539.0-61.44444444-8569.5-17673.125-0.0-0-1-0-0-0-0-1.0-0-202-73403.0-1460-0.0-0.0 to 372-352-3436975.32352941-58082282.0-309.7058823529-59.2222222222-7288026.0625-6873950.64705882-221407.0-0-1-0-0-0-0-1-0-281-58211839-1460-255512-187302\n",
      "Columns: 22 entries, Fwd Header Len to Active Min\n",
      "dtypes: float64(22)\n",
      "memory usage: 524.5+ MB\n",
      "None\n",
      "========================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# 1. Carregamento e Limpeza \n",
    "# ==========================================\n",
    "csv_files = [\n",
    "    \"novods/Friday-02-03-2018_TrafficForML_CICFlowMeter.csv\",\n",
    "    \"novods/Friday-16-02-2018_TrafficForML_CICFlowMeter.csv\",\n",
    "    \"novods/Friday-23-02-2018_TrafficForML_CICFlowMeter.csv\",\n",
    "    \"novods/Thursday-01-03-2018_TrafficForML_CICFlowMeter.csv\",\n",
    "    \"novods/Thursday-15-02-2018_TrafficForML_CICFlowMeter.csv\",\n",
    "    #caso queira carregar os outros arquivos, só descomentar,\n",
    "    #mas vai precisar alterar os testes para refletir a realidade dos ataques majoritarios e minoritarios\n",
    "    #\"novods/Thursday-22-02-2018_TrafficForML_CICFlowMeter.csv\", s\n",
    "    #\"novods/Wednesday-14-02-2018_TrafficForML_CICFlowMeter.csv\",\n",
    "    #\"novods/Wednesday-21-02-2018_TrafficForML_CICFlowMeter.csv\",\n",
    "    #\"novods/Wednesday-28-02-2018_TrafficForML_CICFlowMeter.csv\",\n",
    "]\n",
    "\n",
    "dataframes_list = []\n",
    "for file_path in csv_files: #ler os arquivos\n",
    "    df_temp = pd.read_csv(file_path, low_memory=False) #salvar em um dataframe temporario\n",
    "    df_temp.drop_duplicates(inplace=True) #dropar as duplicatas\n",
    "    dataframes_list.append(df_temp) #lista dos dataframes\n",
    "\n",
    "print(\"Concatenando datasets...\") #concatenar os datasets\n",
    "dataset_ids = pd.concat(dataframes_list, ignore_index=True) #\n",
    "dataset_ids.columns = dataset_ids.columns.str.strip() \n",
    "\n",
    "# Seleção de Features\n",
    "feature_columns = [\n",
    "    \"Label\", \"Protocol\", \"Fwd Header Len\", \"Bwd Header Len\", \"Flow IAT Mean\",\n",
    "    \"Idle Mean\", \"Bwd Pkt Len Mean\", \"Fwd Pkt Len Mean\", \"Bwd IAT Mean\", \"Fwd IAT Mean\",\n",
    "    \"Active Mean\", \"SYN Flag Cnt\", \"PSH Flag Cnt\", \"ACK Flag Cnt\", \"URG Flag Cnt\",\n",
    "    \"FIN Flag Cnt\", \"Fwd Pkt Len Min\", \"Flow IAT Min\", \"Pkt Len Min\", \"Fwd Pkt Len Max\",\n",
    "    \"Flow IAT Max\", \"Pkt Len Max\", \"Active Max\", \"Active Min\"\n",
    "]\n",
    "\n",
    "features = dataset_ids[feature_columns].copy()\n",
    "\n",
    "# Criação do Flow_Label (ID único do fluxo)\n",
    "columns_to_concat = [col for col in feature_columns if col not in ['Label', 'Protocol']]\n",
    "features[\"Flow_Label\"] = features[columns_to_concat].astype(str).agg('-'.join, axis=1)\n",
    "features[\"Flow_Label\"] = features[\"Flow_Label\"].str.lower().str.strip()\n",
    "\n",
    "# Preparação Final (Remover duplicatas de Flow_Label para garantir unicidade do \"usuário\")\n",
    "features = features.drop_duplicates(subset=['Flow_Label'])\n",
    "features = features.set_index(\"Flow_Label\")\n",
    "\n",
    "# Separar X (features numéricas) e y (Rótulo)\n",
    "X_raw = features.drop(columns=['Label', 'Protocol'], errors='ignore')\n",
    "y_raw = features['Label']\n",
    "\n",
    "print(\"Sanitizando dados numéricos (removendo cabeçalhos repetidos e strings inválidas)...\")\n",
    "# Força a conversão para numérico. Se encontrar string ('Fwd Header Len'), vira NaN.\n",
    "X_raw = X_raw.apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "# Verifica quais linhas têm NaN (eram as linhas com lixo/cabeçalho)\n",
    "rows_with_nan = X_raw.isnull().any(axis=1)\n",
    "if rows_with_nan.sum() > 0:\n",
    "    print(f\"   AVISO: Removendo {rows_with_nan.sum()} linhas inválidas que não puderam ser convertidas para float.\")\n",
    "    # Remove essas linhas tanto do X quanto do y para manter a consistência\n",
    "    X_raw = X_raw[~rows_with_nan]\n",
    "    y_raw = y_raw[~rows_with_nan]\n",
    "\n",
    "print(\"Dados sanitizados com sucesso.\")\n",
    "\n",
    "# --- NOVO: Descrição do Dataset (Pós-Sanitização) ---\n",
    "print(\"\\n\" + \"=\"*40)\n",
    "print(\"       RESUMO DO DATASET (Limpo)       \")\n",
    "print(\"=\"*40)\n",
    "print(f\"Shape Total: {X_raw.shape} (Linhas, Colunas)\")\n",
    "print(\"\\nDistribuição de Rótulos (Top 20):\")\n",
    "print(y_raw.value_counts().head(20))\n",
    "print(\"\\nInformações das Colunas:\")\n",
    "print(X_raw.info(verbose=False))\n",
    "print(\"=\"*40 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d0b64136",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ==========================================\n",
    "# 2. Classe Otimizada com FAISS\n",
    "# ==========================================\n",
    "class FAISS_IDS_Recommender:\n",
    "    def __init__(self, k_neighbors=50):\n",
    "        self.k = k_neighbors\n",
    "        self.index = None\n",
    "        self.train_labels = None\n",
    "        self.scaler = StandardScaler()\n",
    "    \n",
    "    def fit(self, X_train, y_train):\n",
    "        print(f\"   [FAISS] Iniciando fit com {len(X_train)} amostras...\")\n",
    "        if len(X_train) == 0:\n",
    "            raise ValueError(\"O conjunto de treino está vazio! Verifique a divisão dos dados.\")\n",
    "\n",
    "        # 1. Scaling\n",
    "        X_scaled = self.scaler.fit_transform(X_train).astype('float32')\n",
    "        \n",
    "        # --- CORREÇÃO: Garantir array C-Contiguous para o FAISS ---\n",
    "        X_scaled = np.ascontiguousarray(X_scaled)\n",
    "        # ---------------------------------------------------------\n",
    "        \n",
    "        # 2. Normalização L2\n",
    "        faiss.normalize_L2(X_scaled)\n",
    "        \n",
    "        # 3. Criação do Índice FAISS\n",
    "        d = X_scaled.shape[1]\n",
    "        self.index = faiss.IndexFlatIP(d)\n",
    "        self.index.add(X_scaled)\n",
    "        \n",
    "        # 4. Lookup Array\n",
    "        self.train_labels = y_train.to_numpy()\n",
    "        print(\"   [FAISS] Índice construído.\")\n",
    "        \n",
    "    def predict(self, X_test):\n",
    "        print(f\"   [FAISS] Iniciando busca para {len(X_test)} amostras de teste...\")\n",
    "        if len(X_test) == 0:\n",
    "            return []\n",
    "\n",
    "        # 1. Preprocessamento do Teste\n",
    "        X_test_scaled = self.scaler.transform(X_test).astype('float32')\n",
    "\n",
    "        # --- CORREÇÃO: Garantir array C-Contiguous para o FAISS ---\n",
    "        X_test_scaled = np.ascontiguousarray(X_test_scaled)\n",
    "        # ---------------------------------------------------------\n",
    "\n",
    "        faiss.normalize_L2(X_test_scaled)\n",
    "        \n",
    "        # 2. Busca\n",
    "        # Se k for maior que o número de amostras no treino, reduz k\n",
    "        k_search = min(self.k, len(self.train_labels))\n",
    "        D, I = self.index.search(X_test_scaled, k_search)\n",
    "        \n",
    "        # 3. Atribuição de Rótulos\n",
    "        neighbors_labels = self.train_labels[I]\n",
    "        \n",
    "        # 4. Votação\n",
    "        classified_attacks = []\n",
    "        for row_labels in tqdm(neighbors_labels, desc=\"   [FAISS] Classificando\"):\n",
    "            counts = Counter(row_labels)\n",
    "            # Retorna lista ordenada por score\n",
    "            sorted_attacks = [lbl for lbl, cnt in counts.most_common()]\n",
    "            classified_attacks.append(sorted_attacks)\n",
    "            \n",
    "        return classified_attacks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e67e7802",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ==========================================\n",
    "# 3. Função de Execução dos Cenários\n",
    "# ==========================================\n",
    "def run_test_scenario(test_name, train_split_map, full_data_X, full_data_y):\n",
    "    print(f\"\\n{'='*60}\\nIniciando {test_name}\\n{'='*60}\")\n",
    "    \n",
    "    train_idxs = []\n",
    "    test_idxs = []\n",
    "    \n",
    "    # --- Divisão de Dados (Customizada por Classe) ---\n",
    "    unique_labels = full_data_y.unique()\n",
    "    \n",
    "    # Verifica quais labels existem no split map, se não tiver, assume 0.3 (default)\n",
    "    # Se o split_map for vazio, trata de forma especial (Teste 3)\n",
    "    \n",
    "    for label in unique_labels:\n",
    "        # Pega índices dessa classe\n",
    "        indices = full_data_y[full_data_y == label].index\n",
    "        \n",
    "        if len(indices) < 2:\n",
    "            # Se só tem 1 amostra, vai pro treino (segurança)\n",
    "            train_idxs.extend(indices)\n",
    "            continue\n",
    "            \n",
    "        test_size = train_split_map.get(label, 0.3) # Default 30% teste\n",
    "        \n",
    "        # Se test_size for > 1, assume-se que é contagem absoluta? Não, vamos assumir ratio.\n",
    "        # Caso especial para Teste 3 onde o split é global ou diferente\n",
    "        \n",
    "        try:\n",
    "            train_i, test_i = train_test_split(indices, test_size=test_size, random_state=42)\n",
    "            train_idxs.extend(train_i)\n",
    "            test_idxs.extend(test_i)\n",
    "        except ValueError:\n",
    "            # Fallback para classes muito pequenas\n",
    "            train_idxs.extend(indices)\n",
    "\n",
    "    # Criação dos DataFrames Finais\n",
    "    X_train = full_data_X.loc[train_idxs]\n",
    "    y_train = full_data_y.loc[train_idxs]\n",
    "    \n",
    "    # Ajuste para o Teste 3 (onde Teste = Todo o dataset, ou 90%)\n",
    "    if \"Teste 3\" in test_name:\n",
    "        # No Teste 3 do artigo: Treino 10%, Teste 90%\n",
    "        # A lógica acima já dividiu. Se o map passou 0.9 para teste, está correto.\n",
    "        X_test = full_data_X.loc[test_idxs]\n",
    "        y_test = full_data_y.loc[test_idxs]\n",
    "    elif \"Teste 2\" in test_name:\n",
    "         # No Teste 2: Treino só tem algumas classes, Teste tem todas\n",
    "         # A lógica acima monta teste baseada no complemento do treino.\n",
    "         # Para garantir que o teste tenha TUDO que não é treino:\n",
    "         X_test = full_data_X.loc[test_idxs]\n",
    "         y_test = full_data_y.loc[test_idxs]\n",
    "    else:\n",
    "        X_test = full_data_X.loc[test_idxs]\n",
    "        y_test = full_data_y.loc[test_idxs]\n",
    "\n",
    "    print(f\"Tamanho Treino: {len(X_train)} | Tamanho Teste: {len(X_test)}\")\n",
    "    \n",
    "    # --- Execução do Modelo FAISS ---\n",
    "    recommender = FAISS_IDS_Recommender(k_neighbors=50)\n",
    "    recommender.fit(X_train, y_train)\n",
    "    y_pred_lists = recommender.predict(X_test)\n",
    "    \n",
    "    # --- Avaliação ---\n",
    "    # Prepara dados para avaliação Multi-Label\n",
    "    # y_test é uma Series de strings, precisamos converter para lista de listas para o MLB\n",
    "    y_true_lists = [[label] for label in y_test]\n",
    "    \n",
    "    # O y_pred_lists já é uma lista de listas (recomendações)\n",
    "    # Mas para avaliação estrita (acertou o ataque exato?), pegamos o Top-1\n",
    "    # O artigo menciona \"relatórios de classificação multi-label\"\n",
    "    \n",
    "    # Coletar todas as classes possíveis\n",
    "    all_classes = sorted(list(set(y_train.unique()) | set(y_test.unique())))\n",
    "    mlb = MultiLabelBinarizer(classes=all_classes)\n",
    "    \n",
    "    # Para bater com as tabelas do artigo, geralmente se avalia se o Rótulo Real estava nas recomendações\n",
    "    # Ou se a recomendação Top-1 bate. Vamos assumir Top-1 para precisão direta ou \n",
    "    # usar a saída completa do recommender como \"predição\".\n",
    "    \n",
    "    # Ajuste: O artigo compara \"Label Real\" vs \"Classificação\".\n",
    "    # Como é um IDS, vamos considerar a predição principal (índice 0 da lista recomendada)\n",
    "    y_pred_top1 = [[preds[0]] if preds else [] for preds in y_pred_lists]\n",
    "    \n",
    "    y_true_bin = mlb.fit_transform(y_true_lists)\n",
    "    y_pred_bin = mlb.transform(y_pred_top1) # Avaliando Top-1\n",
    "    \n",
    "    print(f\"\\n--- Relatório de Classificação (Top-1 Recommendation) para {test_name} ---\")\n",
    "    report = classification_report(y_true_bin, y_pred_bin, target_names=mlb.classes_, zero_division=0)\n",
    "    print(report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffd4e55e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Iniciando Teste 1: 70% Treino / 30% Teste)\n",
      "============================================================\n",
      "Tamanho Treino: 2092329 | Tamanho Teste: 896719\n",
      "   [FAISS] Iniciando fit com 2092329 amostras...\n",
      "   [FAISS] Índice construído.\n",
      "   [FAISS] Iniciando busca para 896719 amostras de teste...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   [FAISS] Classificando: 100%|██████████| 896719/896719 [00:02<00:00, 312870.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Relatório de Classificação (Top-1 Recommendation) para Teste 1: 70% Treino / 30% Teste) ---\n",
      "                          precision    recall  f1-score   support\n",
      "\n",
      "                  Benign       0.98      1.00      0.99    778934\n",
      "                     Bot       1.00      0.99      1.00     43191\n",
      "        Brute Force -Web       0.92      0.24      0.38       102\n",
      "        Brute Force -XSS       1.00      0.40      0.57        43\n",
      "   DoS attacks-GoldenEye       0.98      1.00      0.99     12407\n",
      "        DoS attacks-Hulk       1.00      1.00      1.00     43503\n",
      "DoS attacks-SlowHTTPTest       0.71      1.00      0.83        17\n",
      "   DoS attacks-Slowloris       0.96      0.99      0.98      2958\n",
      "           Infilteration       0.84      0.10      0.17     15551\n",
      "           SQL Injection       0.00      0.00      0.00        13\n",
      "\n",
      "               micro avg       0.98      0.98      0.98    896719\n",
      "               macro avg       0.84      0.67      0.69    896719\n",
      "            weighted avg       0.98      0.98      0.98    896719\n",
      "             samples avg       0.98      0.98      0.98    896719\n",
      "\n",
      "\n",
      "============================================================\n",
      "Iniciando Teste 2: Ataques Menos Comuns\n",
      "============================================================\n",
      "Tamanho Treino: 7304 | Tamanho Teste: 3133\n",
      "   [FAISS] Iniciando fit com 7304 amostras...\n",
      "   [FAISS] Índice construído.\n",
      "   [FAISS] Iniciando busca para 3133 amostras de teste...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   [FAISS] Classificando: 100%|██████████| 3133/3133 [00:00<00:00, 214690.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Relatório de Classificação (Top-1 Recommendation) para Teste 2: Ataques Menos Comuns ---\n",
      "                          precision    recall  f1-score   support\n",
      "\n",
      "        Brute Force -Web       0.80      0.93      0.86       102\n",
      "        Brute Force -XSS       0.94      0.67      0.78        43\n",
      "DoS attacks-SlowHTTPTest       1.00      1.00      1.00        17\n",
      "   DoS attacks-Slowloris       1.00      1.00      1.00      2958\n",
      "           SQL Injection       0.00      0.00      0.00        13\n",
      "\n",
      "               micro avg       0.99      0.99      0.99      3133\n",
      "               macro avg       0.75      0.72      0.73      3133\n",
      "            weighted avg       0.99      0.99      0.99      3133\n",
      "             samples avg       0.99      0.99      0.99      3133\n",
      "\n",
      "\n",
      "============================================================\n",
      "Iniciando Teste 3: Treino de 25% das classes majoritárias, Teste com todo o dataset\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamanho Treino: 2733439 | Tamanho Teste: 255609\n",
      "   [FAISS] Iniciando fit com 2733439 amostras...\n",
      "   [FAISS] Índice construído.\n",
      "   [FAISS] Iniciando busca para 255609 amostras de teste...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   [FAISS] Classificando: 100%|██████████| 255609/255609 [00:01<00:00, 242170.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Relatório de Classificação (Top-1 Recommendation) para Teste 3: Treino de 25% das classes majoritárias, Teste com todo o dataset ---\n",
      "                          precision    recall  f1-score   support\n",
      "\n",
      "                  Benign       0.00      0.00      0.00         0\n",
      "                     Bot       1.00      0.99      1.00    107976\n",
      "        Brute Force -Web       0.00      0.00      0.00         0\n",
      "        Brute Force -XSS       0.00      0.00      0.00         0\n",
      "   DoS attacks-GoldenEye       0.00      0.00      0.00         0\n",
      "        DoS attacks-Hulk       1.00      1.00      1.00    108757\n",
      "DoS attacks-SlowHTTPTest       0.00      0.00      0.00         0\n",
      "   DoS attacks-Slowloris       0.00      0.00      0.00         0\n",
      "           Infilteration       1.00      0.07      0.13     38876\n",
      "           SQL Injection       0.00      0.00      0.00         0\n",
      "\n",
      "               micro avg       0.85      0.85      0.85    255609\n",
      "               macro avg       0.30      0.21      0.21    255609\n",
      "            weighted avg       1.00      0.85      0.87    255609\n",
      "             samples avg       0.85      0.85      0.85    255609\n",
      "\n",
      "\n",
      "============================================================\n",
      "Iniciando Teste 4: 10% Treino / 90% Teste (Todos os Dados)\n",
      "============================================================\n",
      "Tamanho Treino: 298899 | Tamanho Teste: 2690149\n",
      "   [FAISS] Iniciando fit com 298899 amostras...\n",
      "   [FAISS] Índice construído.\n",
      "   [FAISS] Iniciando busca para 2690149 amostras de teste...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ==========================================\n",
    "# 4. Definição e Execução dos Testes\n",
    "# ==========================================\n",
    "\n",
    "# CENÁRIO 1: (70% Treino / 30% Teste)\n",
    "# Todos as classes divididas igual\n",
    "map_1 = {label: 0.3 for label in y_raw.unique()}\n",
    "run_test_scenario(\"Teste 1: 70% Treino / 30% Teste)\", map_1, X_raw, y_raw)\n",
    "\n",
    "# CENÁRIO 2: Ataques Menos Comuns\n",
    "# selecionados os 5 ataques menos frequentes 70% treino 30% teste\"\n",
    "minority_attacks = [\n",
    "    'Brute Force -Web', 'Brute Force -XSS', 'DoS attacks-SlowHTTPTest', \n",
    "    'DoS attacks-Slowloris', 'SQL Injection'\n",
    "]\n",
    "\n",
    "# Filtrando dados\n",
    "mask_minority = y_raw.isin(minority_attacks)\n",
    "X_minor = X_raw[mask_minority]\n",
    "y_minor = y_raw[mask_minority]\n",
    "map_2 = {label: 0.3 for label in y_minor.unique()}\n",
    "\n",
    "run_test_scenario(\"Teste 2: Ataques Menos Comuns\", map_2, X_minor, y_minor)\n",
    "\n",
    "\n",
    "# CENÁRIO 3: Teste 2 do Artigo (Treino Desbalanceado, Teste Total)\n",
    "# \"Treino composto exclusivamente por amostras dos 3 ataques com maior número... \n",
    "# divisão de 25% para treino (desses 3) e o resto teste\"\n",
    "major_attacks = ['DoS attacks-Hulk', 'Infilteration', 'Bot'] \n",
    "map_3 = {}\n",
    "for label in y_raw.unique():\n",
    "    if label in major_attacks:\n",
    "        map_3[label] = 0.75 # 25% Treino, 75% Teste\n",
    "    else:\n",
    "        map_3[label] = 1.0 # 0% Treino (100% Teste) - Simula que não conhece os outros\n",
    "\n",
    "run_test_scenario(\"Teste 3: Treino de 25% das classes majoritárias, Teste com todo o dataset\", map_3, X_raw, y_raw)\n",
    "\n",
    "\n",
    "# CENÁRIO 4: Teste 3 do Artigo (10% Treino, 90% Teste Global)\n",
    "map_4 = {label: 0.90 for label in y_raw.unique()} # 10% Treino, 90% Teste\n",
    "run_test_scenario(\"Teste 4: 10% Treino / 90% Teste (Todos os Dados)\", map_4, X_raw, y_raw)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d5d7034",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import faiss  \n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MultiLabelBinarizer\n",
    "from sklearn.metrics import classification_report, multilabel_confusion_matrix\n",
    "from tqdm import tqdm\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2896c2bd",
   "metadata": {},
   "source": [
    "#Carregamento e Limpeza "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64f8024e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concatenando datasets...\n",
      "Sanitizando dados numéricos (removendo cabeçalhos repetidos e strings inválidas)...\n",
      "   AVISO: Removendo 1 linhas inválidas que não puderam ser convertidas para float.\n",
      "Dados limpos.\n",
      "\n",
      "========================================\n",
      "       RESUMO DO DATASET      \n",
      "========================================\n",
      "Shape Total: (2989048, 22) (Linhas, Colunas)\n",
      "\n",
      "Distribuição de Rótulos (Top 20):\n",
      "Label\n",
      "Benign                      2596445\n",
      "DoS attacks-Hulk             145009\n",
      "Bot                          143967\n",
      "Infilteration                 51834\n",
      "DoS attacks-GoldenEye         41356\n",
      "DoS attacks-Slowloris          9859\n",
      "Brute Force -Web                337\n",
      "Brute Force -XSS                143\n",
      "DoS attacks-SlowHTTPTest         55\n",
      "SQL Injection                    43\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Informações das Colunas:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 2989048 entries, 192-152-9425.666667-0.0-539.0-61.44444444-8569.5-17673.125-0.0-0-1-0-0-0-0-1.0-0-202-73403.0-1460-0.0-0.0 to 372-352-3436975.32352941-58082282.0-309.7058823529-59.2222222222-7288026.0625-6873950.64705882-221407.0-0-1-0-0-0-0-1-0-281-58211839-1460-255512-187302\n",
      "Columns: 22 entries, Fwd Header Len to Active Min\n",
      "dtypes: float64(22)\n",
      "memory usage: 524.5+ MB\n",
      "None\n",
      "========================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "csv_files = [\n",
    "    \"novods/Friday-02-03-2018_TrafficForML_CICFlowMeter.csv\",\n",
    "    \"novods/Friday-16-02-2018_TrafficForML_CICFlowMeter.csv\",\n",
    "    \"novods/Friday-23-02-2018_TrafficForML_CICFlowMeter.csv\",\n",
    "    \"novods/Thursday-01-03-2018_TrafficForML_CICFlowMeter.csv\",\n",
    "    \"novods/Thursday-15-02-2018_TrafficForML_CICFlowMeter.csv\",\n",
    "    #caso queira carregar os outros arquivos, só descomentar,\n",
    "    #mas vai precisar alterar os testes para refletir a realidade dos ataques majoritarios e minoritarios\n",
    "    #\"novods/Thursday-22-02-2018_TrafficForML_CICFlowMeter.csv\", s\n",
    "    #\"novods/Wednesday-14-02-2018_TrafficForML_CICFlowMeter.csv\",\n",
    "    #\"novods/Wednesday-21-02-2018_TrafficForML_CICFlowMeter.csv\",\n",
    "    #\"novods/Wednesday-28-02-2018_TrafficForML_CICFlowMeter.csv\",\n",
    "]\n",
    "\n",
    "dataframes_list = []\n",
    "for file_path in csv_files: #ler os arquivos\n",
    "    df_temp = pd.read_csv(file_path, low_memory=False) #salvar em um dataframe temporario\n",
    "    df_temp.drop_duplicates(inplace=True) #dropar as duplicatas\n",
    "    dataframes_list.append(df_temp) #lista dos dataframes\n",
    "\n",
    "print(\"Concatenando datasets...\") #concatenar os datasets\n",
    "dataset_ids = pd.concat(dataframes_list, ignore_index=True) #\n",
    "dataset_ids.columns = dataset_ids.columns.str.strip() \n",
    "\n",
    "# Seleção de Features\n",
    "feature_columns = [\n",
    "    \"Label\", \"Protocol\", \"Fwd Header Len\", \"Bwd Header Len\", \"Flow IAT Mean\",\n",
    "    \"Idle Mean\", \"Bwd Pkt Len Mean\", \"Fwd Pkt Len Mean\", \"Bwd IAT Mean\", \"Fwd IAT Mean\",\n",
    "    \"Active Mean\", \"SYN Flag Cnt\", \"PSH Flag Cnt\", \"ACK Flag Cnt\", \"URG Flag Cnt\",\n",
    "    \"FIN Flag Cnt\", \"Fwd Pkt Len Min\", \"Flow IAT Min\", \"Pkt Len Min\", \"Fwd Pkt Len Max\",\n",
    "    \"Flow IAT Max\", \"Pkt Len Max\", \"Active Max\", \"Active Min\"\n",
    "]\n",
    "\n",
    "features = dataset_ids[feature_columns].copy()\n",
    "\n",
    "# Criação do Flow_Label (ID único do fluxo)\n",
    "columns_to_concat = [col for col in feature_columns if col not in ['Label', 'Protocol']]\n",
    "features[\"Flow_Label\"] = features[columns_to_concat].astype(str).agg('-'.join, axis=1)\n",
    "features[\"Flow_Label\"] = features[\"Flow_Label\"].str.lower().str.strip()\n",
    "\n",
    "features = features.drop_duplicates(subset=['Flow_Label'])\n",
    "features = features.set_index(\"Flow_Label\")\n",
    "\n",
    "# Separar X (features numéricas) e y (Rótulo)\n",
    "X_raw = features.drop(columns=['Label', 'Protocol'], errors='ignore')\n",
    "y_raw = features['Label']\n",
    "\n",
    "print(\"Sanitizando dados numéricos (removendo cabeçalhos repetidos e strings inválidas)...\")\n",
    "# converte tudo pra numerico, pra nao ter uma string inesperada la no meio\n",
    "X_raw = X_raw.apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "# limpa as linhas com NaN\n",
    "rows_with_nan = X_raw.isnull().any(axis=1)\n",
    "if rows_with_nan.sum() > 0:\n",
    "    print(f\"   AVISO: Removendo {rows_with_nan.sum()} linhas inválidas que não puderam ser convertidas para float.\")\n",
    "    # Remove essas linhas tanto do X quanto do y para manter a consistência\n",
    "    X_raw = X_raw[~rows_with_nan]\n",
    "    y_raw = y_raw[~rows_with_nan]\n",
    "\n",
    "print(\"Dados limpos.\")\n",
    "\n",
    "# descricao do dataset (numero de entradas por label)\n",
    "print(\"\\n\" + \"=\"*40)\n",
    "print(\"       RESUMO DO DATASET      \")\n",
    "print(\"=\"*40)\n",
    "print(f\"Shape Total: {X_raw.shape} (Linhas, Colunas)\")\n",
    "print(\"\\nDistribuição de Rótulos (Top 20):\")\n",
    "print(y_raw.value_counts().head(20))\n",
    "print(\"\\nInformações das Colunas:\")\n",
    "print(X_raw.info(verbose=False))\n",
    "print(\"=\"*40 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f717b8fb",
   "metadata": {},
   "source": [
    "#Set-up do FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d0b64136",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FAISS_IDS_Recommender:\n",
    "    def __init__(self, k_neighbors=50):\n",
    "        self.k = k_neighbors\n",
    "        self.index = None\n",
    "        self.train_labels = None\n",
    "        self.scaler = StandardScaler()\n",
    "    \n",
    "    def fit(self, X_train, y_train):\n",
    "        print(f\"   [FAISS] Iniciando fit com {len(X_train)} amostras...\")\n",
    "        if len(X_train) == 0:\n",
    "            raise ValueError(\"conjunto de treino vazio, revisar divisao dos dados\")\n",
    "\n",
    "        # 1. Scaling\n",
    "        X_scaled = self.scaler.fit_transform(X_train).astype('float32')\n",
    "        \n",
    "\n",
    "        #garantir c-contiguous array pro faiss (se nao ele quebra)\n",
    "        X_scaled = np.ascontiguousarray(X_scaled)\n",
    "\n",
    "        #normalizacao l2\n",
    "        faiss.normalize_L2(X_scaled)\n",
    "        \n",
    "        # 3. Criação do Índice FAISS\n",
    "        d = X_scaled.shape[1]\n",
    "        self.index = faiss.IndexFlatIP(d)\n",
    "        self.index.add(X_scaled)\n",
    "        \n",
    "        # 4. Lookup Array\n",
    "        self.train_labels = y_train.to_numpy()\n",
    "        print(\"   [FAISS] Índice construído.\")\n",
    "        \n",
    "    def predict(self, X_test):\n",
    "        print(f\"   [FAISS] Iniciando busca para {len(X_test)} amostras de teste...\")\n",
    "        if len(X_test) == 0:\n",
    "            return []\n",
    "\n",
    "        # 1. Preprocessamento do Teste\n",
    "        X_test_scaled = self.scaler.transform(X_test).astype('float32')\n",
    "\n",
    "        # garantir c-contiguous array pro faiss (se nao ele quebra)\n",
    "        X_test_scaled = np.ascontiguousarray(X_test_scaled)\n",
    "\n",
    "        #normalizacao l2\n",
    "        faiss.normalize_L2(X_test_scaled)\n",
    "        \n",
    "        # se k for maior doq o numero de amostras, reduz k\n",
    "        k_search = min(self.k, len(self.train_labels))\n",
    "\n",
    "        D, I = self.index.search(X_test_scaled, k_search)\n",
    "        \n",
    "        # 3. Atribuição de Rótulos\n",
    "        neighbors_labels = self.train_labels[I]\n",
    "        \n",
    "        # 4. Votação\n",
    "        classified_attacks = []\n",
    "        for row_labels in tqdm(neighbors_labels, desc=\"   [FAISS] Classificando\"):\n",
    "            counts = Counter(row_labels)\n",
    "            # Retorna lista ordenada por score\n",
    "            sorted_attacks = [lbl for lbl, cnt in counts.most_common()]\n",
    "            classified_attacks.append(sorted_attacks)\n",
    "            \n",
    "        return classified_attacks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80a1c284",
   "metadata": {},
   "source": [
    "#função para executar os cenários"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e67e7802",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_test_scenario(test_name, test_size_map, full_data_X, full_data_y):\n",
    "    print(f\"\\n{'='*60}\\nIniciando {test_name}\\n{'='*60}\")\n",
    "    \n",
    "    train_idxs = []\n",
    "    test_idxs = []\n",
    "    \n",
    "    # --- divisao de dados --- \n",
    "    unique_labels = full_data_y.unique()\n",
    "    \n",
    "    for label in unique_labels:\n",
    "        # para cada classe, pega os indices dela\n",
    "        indices = full_data_y[full_data_y == label].index \n",
    "        \n",
    "        if len(indices) < 2:\n",
    "            # se só tem 1 amostra, vai pro treino (sanity test)\n",
    "            train_idxs.extend(indices)\n",
    "            continue\n",
    "            \n",
    "        test_size = test_size_map.get(label, 0.3)\n",
    "        \n",
    "        if test_size >= 1.0:\n",
    "            # 100% Teste, 0% Treino\n",
    "            test_idxs.extend(indices)\n",
    "            continue\n",
    "            \n",
    "        if test_size <= 0.0:\n",
    "            # 0% Teste, 100% Treino\n",
    "            train_idxs.extend(indices)\n",
    "            continue\n",
    "            \n",
    "        try:\n",
    "            train_i, test_i = train_test_split(indices, test_size=test_size, random_state=42)\n",
    "            train_idxs.extend(train_i)\n",
    "            test_idxs.extend(test_i)\n",
    "        except ValueError:\n",
    "            train_idxs.extend(indices)\n",
    "\n",
    "    # cria os dataframes de treino e teste com os indices que pegamos no passo anterior\n",
    "    X_train = full_data_X.loc[train_idxs]\n",
    "    y_train = full_data_y.loc[train_idxs]\n",
    "    \n",
    "    X_test = full_data_X.loc[test_idxs]\n",
    "    y_test = full_data_y.loc[test_idxs]\n",
    "\n",
    "    print(f\"Tamanho Treino: {len(X_train)} | Tamanho Teste: {len(X_test)}\")\n",
    "    \n",
    "    # --- executa o faiss ---\n",
    "    recommender = FAISS_IDS_Recommender(k_neighbors=50)\n",
    "    recommender.fit(X_train, y_train)\n",
    "    y_pred_lists = recommender.predict(X_test)\n",
    "    \n",
    "    # --- avaliação ---\n",
    "    # y_test convertido para lista de listas\n",
    "    y_true_lists = [[label] for label in y_test]\n",
    "    \n",
    "    \n",
    "    # coletar todas as classes possíveis\n",
    "    all_classes = sorted(list(set(y_train.unique()) | set(y_test.unique())))\n",
    "    mlb = MultiLabelBinarizer(classes=all_classes)\n",
    "    \n",
    "    #top 1 pred vs real label\n",
    "    y_pred_top1 = [[preds[0]] if preds else [] for preds in y_pred_lists]\n",
    "    \n",
    "    y_true_bin = mlb.fit_transform(y_true_lists)\n",
    "    y_pred_bin = mlb.transform(y_pred_top1) # Avaliando Top-1\n",
    "    \n",
    "    print(f\"\\n--- Relatório de Classificação (Top-1 Recommendation) para {test_name} ---\")\n",
    "    report = classification_report(y_true_bin, y_pred_bin, target_names=mlb.classes_, zero_division=0)\n",
    "    print(report)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dea7f136",
   "metadata": {},
   "source": [
    "#Definição e execução dos testes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ffd4e55e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Iniciando Teste 1: 70% Treino / 30% Teste)\n",
      "============================================================\n",
      "Tamanho Treino: 2092329 | Tamanho Teste: 896719\n",
      "   [FAISS] Iniciando fit com 2092329 amostras...\n",
      "   [FAISS] Índice construído.\n",
      "   [FAISS] Iniciando busca para 896719 amostras de teste...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   [FAISS] Classificando: 100%|██████████| 896719/896719 [00:03<00:00, 297900.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Relatório de Classificação (Top-1 Recommendation) para Teste 1: 70% Treino / 30% Teste) ---\n",
      "                          precision    recall  f1-score   support\n",
      "\n",
      "                  Benign       0.98      1.00      0.99    778934\n",
      "                     Bot       1.00      0.99      1.00     43191\n",
      "        Brute Force -Web       0.92      0.24      0.38       102\n",
      "        Brute Force -XSS       1.00      0.40      0.57        43\n",
      "   DoS attacks-GoldenEye       0.98      1.00      0.99     12407\n",
      "        DoS attacks-Hulk       1.00      1.00      1.00     43503\n",
      "DoS attacks-SlowHTTPTest       0.71      1.00      0.83        17\n",
      "   DoS attacks-Slowloris       0.96      0.99      0.98      2958\n",
      "           Infilteration       0.84      0.10      0.17     15551\n",
      "           SQL Injection       0.00      0.00      0.00        13\n",
      "\n",
      "               micro avg       0.98      0.98      0.98    896719\n",
      "               macro avg       0.84      0.67      0.69    896719\n",
      "            weighted avg       0.98      0.98      0.98    896719\n",
      "             samples avg       0.98      0.98      0.98    896719\n",
      "\n",
      "\n",
      "============================================================\n",
      "Iniciando Teste 2: Ataques Menos Comuns\n",
      "============================================================\n",
      "Tamanho Treino: 7304 | Tamanho Teste: 3133\n",
      "   [FAISS] Iniciando fit com 7304 amostras...\n",
      "   [FAISS] Índice construído.\n",
      "   [FAISS] Iniciando busca para 3133 amostras de teste...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   [FAISS] Classificando: 100%|██████████| 3133/3133 [00:00<00:00, 237588.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Relatório de Classificação (Top-1 Recommendation) para Teste 2: Ataques Menos Comuns ---\n",
      "                          precision    recall  f1-score   support\n",
      "\n",
      "        Brute Force -Web       0.80      0.93      0.86       102\n",
      "        Brute Force -XSS       0.94      0.67      0.78        43\n",
      "DoS attacks-SlowHTTPTest       1.00      1.00      1.00        17\n",
      "   DoS attacks-Slowloris       1.00      1.00      1.00      2958\n",
      "           SQL Injection       0.00      0.00      0.00        13\n",
      "\n",
      "               micro avg       0.99      0.99      0.99      3133\n",
      "               macro avg       0.75      0.72      0.73      3133\n",
      "            weighted avg       0.99      0.99      0.99      3133\n",
      "             samples avg       0.99      0.99      0.99      3133\n",
      "\n",
      "\n",
      "============================================================\n",
      "Iniciando Teste 3: Treino de 25% das classes majoritárias, Teste com todo o dataset\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamanho Treino: 734312 | Tamanho Teste: 2254736\n",
      "   [FAISS] Iniciando fit com 734312 amostras...\n",
      "   [FAISS] Índice construído.\n",
      "   [FAISS] Iniciando busca para 2254736 amostras de teste...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   [FAISS] Classificando: 100%|██████████| 2254736/2254736 [00:08<00:00, 258854.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Relatório de Classificação (Top-1 Recommendation) para Teste 3: Treino de 25% das classes majoritárias, Teste com todo o dataset ---\n",
      "                          precision    recall  f1-score   support\n",
      "\n",
      "                  Benign       0.96      1.00      0.98   1947334\n",
      "                     Bot       1.00      0.99      1.00    107976\n",
      "        Brute Force -Web       0.00      0.00      0.00       337\n",
      "        Brute Force -XSS       0.00      0.00      0.00       143\n",
      "   DoS attacks-GoldenEye       0.00      0.00      0.00     41356\n",
      "        DoS attacks-Hulk       0.96      1.00      0.98    108757\n",
      "DoS attacks-SlowHTTPTest       0.00      0.00      0.00        55\n",
      "   DoS attacks-Slowloris       0.00      0.00      0.00      9859\n",
      "           Infilteration       0.81      0.08      0.15     38876\n",
      "           SQL Injection       0.00      0.00      0.00        43\n",
      "\n",
      "               micro avg       0.96      0.96      0.96   2254736\n",
      "               macro avg       0.37      0.31      0.31   2254736\n",
      "            weighted avg       0.94      0.96      0.94   2254736\n",
      "             samples avg       0.96      0.96      0.96   2254736\n",
      "\n",
      "\n",
      "============================================================\n",
      "Iniciando Teste 4: 10% Treino / 90% Teste (Todos os Dados)\n",
      "============================================================\n",
      "Tamanho Treino: 298899 | Tamanho Teste: 2690149\n",
      "   [FAISS] Iniciando fit com 298899 amostras...\n",
      "   [FAISS] Índice construído.\n",
      "   [FAISS] Iniciando busca para 2690149 amostras de teste...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   [FAISS] Classificando: 100%|██████████| 2690149/2690149 [00:09<00:00, 280061.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Relatório de Classificação (Top-1 Recommendation) para Teste 4: 10% Treino / 90% Teste (Todos os Dados) ---\n",
      "                          precision    recall  f1-score   support\n",
      "\n",
      "                  Benign       0.98      1.00      0.99   2336801\n",
      "                     Bot       1.00      0.99      0.99    129571\n",
      "        Brute Force -Web       0.00      0.00      0.00       304\n",
      "        Brute Force -XSS       0.00      0.00      0.00       129\n",
      "   DoS attacks-GoldenEye       0.94      0.97      0.96     37221\n",
      "        DoS attacks-Hulk       0.99      1.00      0.99    130509\n",
      "DoS attacks-SlowHTTPTest       0.00      0.00      0.00        50\n",
      "   DoS attacks-Slowloris       0.94      0.93      0.93      8874\n",
      "           Infilteration       0.78      0.06      0.11     46651\n",
      "           SQL Injection       0.00      0.00      0.00        39\n",
      "\n",
      "               micro avg       0.98      0.98      0.98   2690149\n",
      "               macro avg       0.56      0.49      0.50   2690149\n",
      "            weighted avg       0.98      0.98      0.97   2690149\n",
      "             samples avg       0.98      0.98      0.98   2690149\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# CENÁRIO 1: (70% Treino / 30% Teste)\n",
    "# Todos as classes divididas igual\n",
    "map_1 = {label: 0.3 for label in y_raw.unique()}\n",
    "run_test_scenario(\"Teste 1: 70% Treino / 30% Teste)\", map_1, X_raw, y_raw)\n",
    "\n",
    "# CENÁRIO 2: Ataques Menos Comuns\n",
    "# selecionados os 5 ataques menos frequentes 70% treino 30% teste\"\n",
    "minority_attacks = [\n",
    "    'Brute Force -Web', 'Brute Force -XSS', 'DoS attacks-SlowHTTPTest', \n",
    "    'DoS attacks-Slowloris', 'SQL Injection'\n",
    "]\n",
    "\n",
    "# Filtrando dados\n",
    "mask_minority = y_raw.isin(minority_attacks)\n",
    "X_minor = X_raw[mask_minority]\n",
    "y_minor = y_raw[mask_minority]\n",
    "map_2 = {label: 0.3 for label in y_minor.unique()}\n",
    "\n",
    "run_test_scenario(\"Teste 2: Ataques Menos Comuns\", map_2, X_minor, y_minor)\n",
    "\n",
    "\n",
    "# CENÁRIO 3: Teste 2 do Artigo (Treino Desbalanceado, Teste Total)\n",
    "# \"Treino composto exclusivamente por amostras das 3 classes com maior número (+ benign)... \n",
    "# divisão de 25% para treino (desses 4) e o resto teste\"\n",
    "major_attacks = ['DoS attacks-Hulk', 'Infilteration', 'Bot', 'Benign'] \n",
    "map_3 = {}\n",
    "for label in y_raw.unique():\n",
    "    if label in major_attacks:\n",
    "        map_3[label] = 0.75 # 25% Treino, 75% Teste\n",
    "    else:\n",
    "        map_3[label] = 1.0 # 0% Treino (100% Teste) - Simula que não conhece os outros\n",
    "\n",
    "run_test_scenario(\"Teste 3: Treino de 25% das classes majoritárias, Teste com todo o dataset\", map_3, X_raw, y_raw)\n",
    "\n",
    "\n",
    "# CENÁRIO 4: Teste 3 do Artigo (10% Treino, 90% Teste Global)\n",
    "map_4 = {label: 0.90 for label in y_raw.unique()} # 10% Treino, 90% Teste\n",
    "run_test_scenario(\"Teste 4: 10% Treino / 90% Teste (Todos os Dados)\", map_4, X_raw, y_raw)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
